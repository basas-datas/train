from transformers import MT5ForConditionalGeneration, MT5Tokenizer, Trainer, TrainingArguments
from datasets import load_dataset
import os
import wandb
import torch

# üîß –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—É—Ç—å
model_name = "google/mt5-large"
run_id = "mt5-large-big_rain_1"
output_dir = f"./{run_id}"
start_batch_size = 50   # ‚ö†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–µ–±–æ–ª—å—à–æ–≥–æ batch, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å OOM
step_batch_size = 1

# üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
model = MT5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = MT5Tokenizer.from_pretrained(model_name)

# üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
data_files = {
    "train": "train.jsonl",
    "validation": "eval.jsonl"
}
dataset = load_dataset("json", data_files=data_files)

# üî† –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def tokenize_function(examples):
    model_inputs = tokenizer(
        examples["text"], max_length=256, truncation=True, padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["target"], max_length=256, truncation=True, padding="max_length"
        )
    # –ó–∞–º–µ–Ω—è–µ–º PAD-—Ç–æ–∫–µ–Ω—ã –Ω–∞ -100, —á—Ç–æ–±—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –∏—Ö –≤ –ø–æ–¥—Å—á—ë—Ç–µ loss
    labels["input_ids"] = [
        [(token if token != tokenizer.pad_token_id else -100) for token in label]
        for label in labels["input_ids"]
    ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# üîë –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è W&B
wandb.login(key="5f028bc0142fb7fa45bdacdde3c00dbbaf8bf98e")

# üöÄ –§—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–∞ batch size
def try_training_with_batch_size(batch_size_start):
    batch_size = batch_size_start
    while batch_size > 0:
        try:
            print(f"\nüöÄ –ü—Ä–æ–±—É–µ–º batch_size = {batch_size}")
            training_args = TrainingArguments(
                output_dir=output_dir,
                evaluation_strategy="steps",
                eval_steps=100,
                learning_rate=3e-5,
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size,
                #fp16=True,  # –í–∫–ª—é—á–∞–π—Ç–µ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ GPU (A100 / V100 / T4)
                num_train_epochs=5,
                logging_steps=100,
                warmup_ratio=0.06,
                logging_first_step=True,
                weight_decay=0.01,
                logging_dir="./logs",
                save_total_limit=3,
                save_strategy="epoch",
                report_to="wandb",
                run_name=run_id,
                disable_tqdm=False,
                max_grad_norm=1.0
            )

            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=tokenized_datasets["train"],
                eval_dataset=tokenized_datasets["validation"]
            )

            trainer.train(resume_from_checkpoint=True)
            return batch_size
        except RuntimeError as e:
            if "CUDA out of memory" in str(e):
                print(f"‚ùå OOM –Ω–∞ batch_size = {batch_size}, —É–º–µ–Ω—å—à–∞–µ–º...")
                torch.cuda.empty_cache()
                batch_size -= step_batch_size
            else:
                raise e
    raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π batch size üò¢")

# üèÅ –ó–∞–ø—É—Å–∫ —Å –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–æ–º
optimal_batch_size = try_training_with_batch_size(start_batch_size)
print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–æ —Å batch_size = {optimal_batch_size}")

# üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")
